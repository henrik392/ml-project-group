# Experiment 9: F2-Optimized Training
# Purpose: Retrain YOLOv11n with hyperparameters optimized for F2 (recall-focused)

experiment_id: exp09_f2_optimized
description: "YOLOv11n retrained with F2-optimized hyperparameters (recall >> precision)"

# Model configuration
model: yolov11n  # YOLOv11 nano
weights: null    # Train from pretrained COCO weights
skip_training: false

# Dataset
data: configs/dataset_fold_0.yaml
fold_id: 0

# Training parameters
epochs: 50       # Increased from 30 (more training time)
imgsz: 1280      # CRITICAL: 4× pixels for small objects (was 640)
batch: 4         # Reduced from 16 due to 4× memory from larger images
device: mps
patience: 5      # FIXED: Actually stop early (was 100)
seed: 42
name: yolov11n_f2_optimized_fold0

# F2-OPTIMIZED HYPERPARAMETERS
# ════════════════════════════════════════════════════════════
# Key insight: F2 weighs recall 5× more than precision
# → Need model to be GENEROUS with detections, not conservative
hyperparameters:
  # Loss weights (CRITICAL CHANGES)
  box: 5.0       # Reduced from 7.5 - less emphasis on perfect box localization
  cls: 2.0       # INCREASED from 0.5 (4×!) - encourage object detection
  dfl: 1.0       # Reduced from 1.5 - less emphasis on box refinement

  # NMS during training (FIXED MISMATCH)
  iou: 0.45      # Match inference NMS (was 0.7 - caused train/inference mismatch)

  # Learning rate schedule (ENABLE DECAY)
  lr0: 0.01      # Initial learning rate
  lrf: 0.001     # Final LR (100× decay, was 0.01 = no decay!)
  cos_lr: true   # Enable cosine annealing (was false)

  # Optimizer settings (keep defaults)
  momentum: 0.937
  weight_decay: 0.0005
  warmup_epochs: 3.0

# Augmentation (keep proven settings from ablation study)
augmentation:
  hsv_h: 0.015   # CRITICAL - ablation showed -34.3% without
  hsv_s: 0.7
  hsv_v: 0.4
  degrees: 0.0   # Disabled - ablation showed -12.7% with rotation
  translate: 0.1
  scale: 0.5
  shear: 0.0
  fliplr: 0.5    # Horizontal flip
  flipud: 0.0    # No vertical flip
  mosaic: 1.0    # Keep mosaic augmentation
  mixup: 0.0
  copy_paste: 0.0

# Inference configuration (use optimal conf from exp03)
inference:
  mode: standard
  conf: 0.01     # Optimal from exp03 conf sweep (F2=0.088)
  iou: 0.45      # Match training NMS

# Tracking
tracking:
  enabled: false

# Evaluation
eval_video_id: video_0

# Notes
notes: |
  F2-OPTIMIZED TRAINING RUN

  Hypothesis:
  ──────────
  Current model (exp02) optimizes for mAP@0.5 (precision-focused).
  F2 requires recall >> precision (β=2 weighs recall 5× more).

  Root Cause Analysis:
  ───────────────────
  1. cls=0.5 vs box=7.5 → Model prioritizes localization over detection
  2. Training NMS iou=0.7 vs inference iou=0.45 → Train/inference mismatch
  3. No LR decay (lrf=0.01) → Poor convergence
  4. imgsz=640 → Small objects only 10-20px

  Key Changes:
  ────────────
  ✓ cls: 0.5→2.0 (4× increase) - Encourage detections
  ✓ box: 7.5→5.0 - Reduce localization emphasis
  ✓ imgsz: 640→1280 - 4× pixels per object
  ✓ iou: 0.7→0.45 - Fix train/inference mismatch
  ✓ lrf: 0.01→0.001 - Enable LR decay
  ✓ patience: 100→5 - Actual early stopping

  Expected Results:
  ────────────────
  Current:  Recall=8.75%, F2=0.088
  Target:   Recall=13-18%, F2=0.13-0.18 (+50-100% improvement)

  If recall improves significantly → Confirms hypothesis that training
  hyperparameters were the bottleneck, not model architecture.
